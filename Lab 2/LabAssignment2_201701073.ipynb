{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAssignment2_201701073.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvilmehta06/Deep-Learning/blob/master/Lab%202/LabAssignment2_201701073.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNJMIIzCgpFv",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2: Deep Learning\n",
        "\n",
        "\n",
        "*   Lab Manual: [Manual Lab 2](https://github.com/purvilmehta06/Deep-Learning/blob/master/Lab%202/Lab_Assignment_2.pdf)\n",
        "*   Author: Purvil Mehta\n",
        "*   Id : 201701073\n",
        "* Github Link : [Deep Learning Course Lab Work Repo](https://github.com/purvilmehta06/Deep-Learning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCTRQ2YwK6pB",
        "colab_type": "text"
      },
      "source": [
        "# Some of the General Function to implement NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjTml6xtKZby",
        "colab_type": "text"
      },
      "source": [
        "## Solve function to handle different activation funtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARbLtAe3IQ6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def solve(method,weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4,result):\n",
        "  if (method==\"Relu\"):\n",
        "    layer1_actOutput = tf.nn.relu(neuralNetwork(weight1,bias1,result))\n",
        "    layer2_actOutput = tf.nn.relu(neuralNetwork(weight2,bias2,layer1_actOutput))\n",
        "    layer3_actOutput = tf.nn.relu(neuralNetwork(weight3,bias3,layer2_actOutput))\n",
        "    output = tf.nn.relu(neuralNetwork(weight4,bias4,layer3_actOutput))\n",
        "  elif (method==\"Softmax\"):\n",
        "    layer1_actOutput = tf.nn.softmax(neuralNetwork(weight1,bias1,result))\n",
        "    layer2_actOutput = tf.nn.softmax(neuralNetwork(weight2,bias2,layer1_actOutput))\n",
        "    layer3_actOutput = tf.nn.softmax(neuralNetwork(weight3,bias3,layer2_actOutput))\n",
        "    output = tf.nn.softmax(neuralNetwork(weight4,bias4,layer3_actOutput))\n",
        "  elif (method==\"tanH\"):\n",
        "    layer1_actOutput = tf.nn.tanh(neuralNetwork(weight1,bias1,result))\n",
        "    layer2_actOutput = tf.nn.tanh(neuralNetwork(weight2,bias2,layer1_actOutput))\n",
        "    layer3_actOutput = tf.nn.tanh(neuralNetwork(weight3,bias3,layer2_actOutput))\n",
        "    output = tf.nn.tanh(neuralNetwork(weight4,bias4,layer3_actOutput))\n",
        "  else:\n",
        "    layer1_actOutput = tf.nn.sigmoid(neuralNetwork(weight1,bias1,result))\n",
        "    layer2_actOutput = tf.nn.sigmoid(neuralNetwork(weight2,bias2,layer1_actOutput))\n",
        "    layer3_actOutput = tf.nn.sigmoid(neuralNetwork(weight3,bias3,layer2_actOutput))\n",
        "    output = tf.nn.sigmoid(neuralNetwork(weight4,bias4,layer3_actOutput))\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034R4kGadbar",
        "colab_type": "text"
      },
      "source": [
        "## Generalise Neural Network Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYbs4UdbO1Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuralNetwork(weight,bias,data):\n",
        "  output = tf.add(tf.matmul(weight,data,transpose_a=True) ,bias)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8KDtOJ02fgi",
        "colab_type": "text"
      },
      "source": [
        "## NN Weight and Bias Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffviCPqO2dAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init(layerOneNode,layerTwoNode,layerThreeNode,batchSize):\n",
        "  weight1 = tf.random.normal([data.shape[0],layerOneNode], mean=0.0, stddev=0.4, dtype=tf.float32,seed=1)\n",
        "  weight2 = tf.random.normal([layerOneNode,layerTwoNode], mean=0.0, stddev=0.4, dtype=tf.float32,seed=1)\n",
        "  weight3 = tf.random.normal([layerTwoNode,layerThreeNode], mean=0.0, stddev=0.4, dtype=tf.float32,seed=1)\n",
        "  weight4 = tf.random.normal([layerThreeNode,layerThreeNode], mean=0.0, stddev=0.4, dtype=tf.float32,seed=1)\n",
        "\n",
        "  bias1 = tf.random.normal([layerOneNode,1],mean=0.0, stddev=0.4,seed=1,dtype=tf.float32,)\n",
        "  bias1 = tf.repeat(bias1, repeats=batchSize,axis=1)\n",
        "\n",
        "  bias2 = tf.random.normal([layerTwoNode,1],mean=0.0, stddev=0.4,seed=1,dtype=tf.float32,)\n",
        "  bias2 = tf.repeat(bias2, repeats=batchSize,axis=1)\n",
        "\n",
        "  bias3 = tf.random.normal([layerThreeNode,1],mean=0.0, stddev=0.4,seed=1,dtype=tf.float32,)\n",
        "  bias3 = tf.repeat(bias3, repeats=batchSize,axis=1)\n",
        "\n",
        "  bias4 = tf.random.normal([layerThreeNode,1],mean=0.0, stddev=0.4,seed=1,dtype=tf.float32,)\n",
        "  bias4 = tf.repeat(bias4, repeats=batchSize,axis=1)\n",
        "  return weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOi13dWhdNTu",
        "colab_type": "text"
      },
      "source": [
        "#MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SONCaamHdTLS",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading from Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HELGhH4v5Fvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9891af68-d603-41c2-bb4d-fa697a8c1b7a"
      },
      "source": [
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "data = tf.Variable(x_train,dtype=tf.float32)\n",
        "data = tf.reshape(data,[60000,784])\n",
        "data = tf.transpose(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex5xqlY9dgir",
        "colab_type": "text"
      },
      "source": [
        "## Implementation on NN with diffirent Activation Function and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWbasCppWz3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "14eab186-8693-4070-e83d-a6c2ded1c202"
      },
      "source": [
        "layerOneNode = 10\n",
        "layerTwoNode = 10\n",
        "layerThreeNode = 10\n",
        "batchSize = 300 #divisible by 60,000\n",
        "totalIteration = data.shape[1]//batchSize\n",
        "Activation_function = ['Relu','Softmax','tanH','Sigmoid'];\n",
        "\n",
        "for k in range(len(Activation_function)):\n",
        "  #calculate weight and bias on random basis\n",
        "  [weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]= init(layerOneNode,layerTwoNode,layerThreeNode,batchSize)\n",
        "\n",
        "  for i in range(1,totalIteration+1):\n",
        "\n",
        "    #Slicing the data matrix into batch size [0-299,300-599 and so on]\n",
        "    ind = tf.range(batchSize*(i-1), batchSize*i, 1) \n",
        "    result = tf.transpose(tf.nn.embedding_lookup(tf.transpose(data), ind))\n",
        "    output = solve(Activation_function[k],weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4,result)\n",
        "    #deciding what is the predicted output\n",
        "    if (i==1):\n",
        "      final = tf.math.argmax(tf.transpose(output),1)\n",
        "    else:\n",
        "      final = tf.concat([final, tf.math.argmax(tf.transpose(output),1)], 0)\n",
        "    \n",
        "  #Generating Confusion Matrix  \n",
        "  conf_matrix = tf.math.confusion_matrix(y_train,final)\n",
        "  print(conf_matrix)\n",
        "  cnt = 0\n",
        "  for i in range(10):\n",
        "    cnt = cnt + conf_matrix[i][i]\n",
        "\n",
        "  accuracy = [cnt/600]\n",
        "  tf.print(\"Accuracy in [\"+str(Activation_function[k])+\"] is\",cnt/600)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   0   91  104  132    0    0    0    0    1 5595]\n",
            " [   0   59  105   49    0    0    0    0    0 6529]\n",
            " [   0  415  552  383    0    0    0    0    3 4605]\n",
            " [   0   58  304  167    0    0    0    0    0 5602]\n",
            " [   0 1036  509   95    0    0    0    0    0 4202]\n",
            " [   0  189  256  210    0    0    0    0    0 4766]\n",
            " [   0  642  130   43    0    0    0    0    1 5102]\n",
            " [   0  522  288   50    0    0    0    0    1 5404]\n",
            " [   0  102  134  120    0    0    0    0    0 5495]\n",
            " [   0  794  498  127    0    0    0    0    0 4530]], shape=(10, 10), dtype=int32)\n",
            "Accuracy in [Relu] is 8.8466666666666658\n",
            "tf.Tensor(\n",
            "[[ 396 4931    3   91   25    0  280  123    9   65]\n",
            " [ 472 5925    3   36   40    0  161  105    0    0]\n",
            " [ 378 4850  193  104   25    0  158  136  105    9]\n",
            " [ 385 5171   72   76   25    0  157   78   49  118]\n",
            " [ 414 5099   11   23   26    0  150  116    2    1]\n",
            " [ 404 4666   56   46   25    0  117   95    1   11]\n",
            " [ 363 5012  182   35   27    1  150  138    3    7]\n",
            " [ 438 5499   24   28   33    0  153   90    0    0]\n",
            " [ 394 4994  153   29   30    1  150   82   14    4]\n",
            " [ 420 5100  101   30   39    0  160   96    0    3]], shape=(10, 10), dtype=int32)\n",
            "Accuracy in [Softmax] is 11.455\n",
            "tf.Tensor(\n",
            "[[1664  256 1289  477    0  880  327  192  663  175]\n",
            " [ 389    5  520 1923    0 1855   67  116 1096  771]\n",
            " [ 634  119  643 1495    0 1633   29  635  430  340]\n",
            " [ 332   15  626 2291    0  932  126  207  964  638]\n",
            " [ 583    2  578 1639    0 1659    9   57  970  345]\n",
            " [1275   80  655  603    0 1232  129  190  714  543]\n",
            " [1840   53 1373  348    0 1511    8  496  196   93]\n",
            " [1220   38  228 1759    0  620  125   31 1077 1167]\n",
            " [ 832   95  295 1681    0 1043   38   90  584 1193]\n",
            " [ 921   18  372 2378    0 1162   15   42  465  576]], shape=(10, 10), dtype=int32)\n",
            "Accuracy in [tanH] is 11.723333333333333\n",
            "tf.Tensor(\n",
            "[[   0    0    0    0    0    0    0    0 5923    0]\n",
            " [   0    0    0    0    0    0    0    0 6742    0]\n",
            " [   0    0    0    0    0    0    0    0 5958    0]\n",
            " [   0    0    0    0    0    0    0    0 6131    0]\n",
            " [   0    0    0    0    0    0    0    0 5842    0]\n",
            " [   0    0    0    0    0    0    0    0 5421    0]\n",
            " [   0    0    0    0    0    0    0    0 5918    0]\n",
            " [   0    0    0    0    0    0    0    0 6265    0]\n",
            " [   0    0    0    0    0    0    0    0 5851    0]\n",
            " [   0    0    0    0    0    0    0    0 5949    0]], shape=(10, 10), dtype=int32)\n",
            "Accuracy in [Sigmoid] is 9.7516666666666669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w2DKeR5Hutwk"
      },
      "source": [
        "## Combination of all\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PzgmBWpiutwl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "31e0a833-0988-4332-b153-8413ea849dbb"
      },
      "source": [
        "layerOneNode = 10\n",
        "layerTwoNode = 10\n",
        "layerThreeNode = 10\n",
        "batchSize = 600 #divisible by 60,000\n",
        "totalIteration = data.shape[1]//batchSize\n",
        "\n",
        "#calculate weight and bias on random basis\n",
        "[weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]= init(layerOneNode,layerTwoNode,layerThreeNode,batchSize)\n",
        "\n",
        "for i in range(1,totalIteration+1):\n",
        "\n",
        "  #Slicing the data matrix into batch size [0-299,300-599 and so on]\n",
        "  ind = tf.range(batchSize*(i-1), batchSize*i, 1) \n",
        "  result = tf.transpose(tf.nn.embedding_lookup(tf.transpose(data), ind))\n",
        "\n",
        "  #Layer 3 implementation with RELU activation function\n",
        "  layer1_actOutput = tf.nn.relu(neuralNetwork(weight1,bias1,result))\n",
        "  layer2_actOutput = tf.nn.tanh(neuralNetwork(weight2,bias2,layer1_actOutput))\n",
        "  layer3_actOutput = tf.nn.relu(neuralNetwork(weight3,bias3,layer2_actOutput))\n",
        "  output = tf.nn.sigmoid(neuralNetwork(weight4,bias4,layer3_actOutput))\n",
        "  \n",
        "  #deciding what is the predicted output\n",
        "  if (i==1):\n",
        "    final = tf.math.argmax(tf.transpose(output),1)\n",
        "  else:\n",
        "    final = tf.concat([final, tf.math.argmax(tf.transpose(output),1)], 0)\n",
        "  \n",
        "#Generating Confusion Matrix  \n",
        "conf_matrix = tf.math.confusion_matrix(y_train,final)\n",
        "print(conf_matrix)\n",
        "cnt = 0\n",
        "for i in range(10):\n",
        "  cnt = cnt + conf_matrix[i][i]\n",
        "\n",
        "accuracy = [cnt/600]\n",
        "tf.print(\"Accuracy : \",cnt/600)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1855   66 1977    0  134  176    0  801  914    0]\n",
            " [ 681    9 5011    0  454   24    0  172  391    0]\n",
            " [1112   29 1540    0  118   75    0 2946  138    0]\n",
            " [ 729    7 3612    0  346  486    0  760  191    0]\n",
            " [ 555   28 1850    1  164   44    0 2990  210    0]\n",
            " [ 661   39 2753    1  504  308    0  814  340    1]\n",
            " [ 467   43 1378    0  305   22    0 3619   84    0]\n",
            " [ 616   31 2409    0  210  118    0 2776  105    0]\n",
            " [ 450   15 3413    0  535  224    0  960  254    0]\n",
            " [1013   33 2266    1  345  105    0 1779  407    0]], shape=(10, 10), dtype=int32)\n",
            "Accuracy :  11.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxK90LFz9SFV",
        "colab_type": "text"
      },
      "source": [
        "## Analysis based on different Batch Size\n",
        "\n",
        "Here I have assumed the activation fuction as \"Sigmoid\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXOdRY9O81Zb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "58983735-98d9-4aaf-840b-1f1b22d01d88"
      },
      "source": [
        "layerOneNode = 10\n",
        "layerTwoNode = 10\n",
        "layerThreeNode = 10\n",
        "batchSize = [200,300,600] #divisible by 60,000\n",
        "\n",
        "\n",
        "for i in range(len(batchSize)):\n",
        "  #calculate weight and bias on random basis\n",
        "  [weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]= init(layerOneNode,layerTwoNode,layerThreeNode,batchSize[i])\n",
        "  \n",
        "  totalIteration = data.shape[1]//batchSize[i]\n",
        "  for j in range(1,totalIteration+1):\n",
        "\n",
        "    #Slicing the data matrix into batch size [0-299,300-599 and so on]\n",
        "    ind = tf.range(batchSize[i]*(j-1), batchSize[i]*j, 1) \n",
        "    result = tf.transpose(tf.nn.embedding_lookup(tf.transpose(data), ind))\n",
        "\n",
        "    #Layer 3 implementation with RELU activation function\n",
        "    layer1_actOutput = tf.nn.sigmoid(neuralNetwork(weight1,bias1,result))\n",
        "    layer2_actOutput = tf.nn.sigmoid(neuralNetwork(weight2,bias2,layer1_actOutput))\n",
        "    layer3_actOutput = tf.nn.sigmoid(neuralNetwork(weight3,bias3,layer2_actOutput))\n",
        "    output = tf.nn.sigmoid(neuralNetwork(weight4,bias4,layer3_actOutput))\n",
        "    \n",
        "    #deciding what is the predicted output\n",
        "    if (j==1):\n",
        "      final = tf.math.argmax(tf.transpose(output),1)\n",
        "    else:\n",
        "      final = tf.concat([final, tf.math.argmax(tf.transpose(output),1)], 0)\n",
        "    \n",
        "  #Generating Confusion Matrix  \n",
        "  conf_matrix = tf.math.confusion_matrix(y_train,final)\n",
        "  cnt = 0\n",
        "  for k in range(10):\n",
        "    cnt = cnt + conf_matrix[k][k]\n",
        "  tf.print(\"Accuracy in batch Size : [\" + str(batchSize[i]) + \"] is\",cnt/600)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy in batch Size : [200] is 9.8716666666666661\n",
            "Accuracy in batch Size : [300] is 9.93\n",
            "Accuracy in batch Size : [600] is 9.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6t_-ZxUDRax",
        "colab_type": "text"
      },
      "source": [
        "#Boston Housing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shjcu5kXDXGS",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading from Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRWlscO5BY3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c1d69b4d-be03-4137-f14e-e0d8b5edef84"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(path='boston_housing.npz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alUnJSVBDkSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = tf.Variable(x_train,dtype=tf.float32)\n",
        "data = tf.transpose(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBu4atYtFFdb",
        "colab_type": "text"
      },
      "source": [
        "## Implementation on Diffirent Activation Function Activation Function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5AtLOaXEGUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "5d861c32-7607-4854-b239-b7a54f01719c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "layerOneNode = 5\n",
        "layerTwoNode = 5\n",
        "layerThreeNode = 1\n",
        "batchSize = [101,202] #divisible by 404\n",
        "Activation_function = ['Relu','Softmax','Sigmoid'];\n",
        "\n",
        "for i in range(len(batchSize)):\n",
        "\n",
        "  for k in range(len(Activation_function)):\n",
        "    #calculate weight and bias on random basis\n",
        "    [weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4]= init(layerOneNode,layerTwoNode,layerThreeNode,batchSize[i])\n",
        "    \n",
        "    totalIteration = data.shape[1]//batchSize[i]\n",
        "    for j in range(1,totalIteration+1):\n",
        "\n",
        "      #Slicing the data matrix into batch size [0-299,300-599 and so on]\n",
        "      ind = tf.range(batchSize[i]*(j-1), batchSize[i]*j, 1) \n",
        "      result = tf.transpose(tf.nn.embedding_lookup(tf.transpose(data), ind))\n",
        "\n",
        "      output = solve(Activation_function[k],weight1,weight2,weight3,weight4,bias1,bias2,bias3,bias4,result)\n",
        "      #deciding what is the predicted output\n",
        "      if (j==1):\n",
        "        final = tf.transpose(output)\n",
        "      else:\n",
        "        final = tf.concat([final, tf.transpose(output)], 0)\n",
        "      \n",
        "\n",
        "    MSE = math.sqrt(tf.math.reduce_sum((final - y_train)**2))\n",
        "    tf.print(\"MSE in batch Size : [\" + str(batchSize[i]) + \"] and activation function [\" + str(Activation_function[k]) + \"] is\",MSE)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE in batch Size : [101] and activation function [Relu] is 9781.143491432891\n",
            "MSE in batch Size : [101] and activation function [Softmax] is 9777.443428626933\n",
            "MSE in batch Size : [101] and activation function [Sigmoid] is 9637.688934594227\n",
            "MSE in batch Size : [202] and activation function [Relu] is 11672.017820411345\n",
            "MSE in batch Size : [202] and activation function [Softmax] is 9779.293225995425\n",
            "MSE in batch Size : [202] and activation function [Sigmoid] is 9624.418943499913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIjGmQifQAjv",
        "colab_type": "text"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqx9YPHYaXns",
        "colab_type": "text"
      },
      "source": [
        "Since we are not propogating backward and calculating weights and bias, it is obvious to have very low accuracy. \n",
        "\n",
        "In the first problem, I tried two approaches.\n",
        "1. Fixed batch size and different activation \n",
        "2. Fixed activation and different batch size \n",
        "\n",
        "In both, we found nearly same accuracy.\n",
        "\n",
        "In the second problem, I tried three activation fuction on two different batch size and found the same result. "
      ]
    }
  ]
}